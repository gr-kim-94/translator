import math
from typing import Optional, Tuple

import torch
from torch import nn


class ScaledDotProductAttention(nn.Module):
    """Scaled dot-product attention as defined in Vaswani et al. (2017)."""

    def __init__(self, dropout: float = 0.0) -> None:
        super().__init__()
        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()
 
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        # q,k,v : (batch, num_heads, seq_len, head_dim)
        head_dim = query.size(-1)
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í–‰ë ¬ ê³±ì…ˆ
        # q_1 * k_2, q_1 * k_3,... ëª¨ë“  í† í°ì—ëŒ€í•´ ë‚´ê³±ì„ í•´ì£¼ëŠ” ë¶€ë¶„.
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(head_dim)
        print("Scaled Scores shape:", scores.shape)

        # ê²°ê³¼: (batch, num_heads, seq_len, seq_len) -> (ëª‡ê°œì˜ ë¬¸ì¥ì¸ì§€, ëª‡ê°œì˜ headì¸ì§€, Q í† í° ìœ„ì¹˜, K í† í° ìœ„ì¹˜)
        if mask is not None:
            dim = mask.dim()
            print("Attention Mask Dim : ", dim, ", Attention Mask Shape : ", mask.shape)
            if dim == 3:
                mask = mask.unsqueeze(1)
            print("Attention New Mask Shape : ", mask.shape)
            scores = scores.masked_fill(mask == 0, float("-inf"))


        # softmax(Q * K^T / sqrt(d_k))
        # softmax ì ìš© (dim=-1 : ë§ˆì§€ë§‰ ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ softmax ì ìš© -> ê° queryì˜ attention weight í•©ì´ 1ì´ ë˜ê²Œ ë§Œë“¦)
        # attentionì—ì„  dimì„ -1ë¡œ ì£¼ë¡œ ì„¤ì •í•¨. ì´ìœ ëŠ” ë§ˆì§€ë§‰ ì°¨ì›ì´ key í† í° ìœ„ì¹˜ì— í•´ë‹¹í•˜ê¸° ë•Œë¬¸.
        # scoresì˜ ë§ˆì§€ë§‰ ì°¨ì›ì€ K í† í° ìœ„ì¹˜ì— í•´ë‹¹.
        attn_weights = torch.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # softmax(Q * K^T / sqrt(d_k)) * V
        output = torch.matmul(attn_weights, value)
        print("Scaled Dot Product Attention Output : ", output[0], "\n====\nAttention Weight : ", attn_weights[0])
        return output, attn_weights


class MultiHeadAttention(nn.Module):
    """Multi-head attention module with learnable projections and output layer."""

    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0) -> None:
        super().__init__()
        if d_model % num_heads != 0:
            # shape ì‚¬ì´ì¦ˆê°€ ê°™ì•„ì•¼í•˜ê¸°ë•Œë¬¸ì—
            # d_modelì„ num_headsë¡œ ë‚˜ëˆ ì„œ ë‚˜ë¨¸ì§€ê°€ 0ì´ ì•„ë‹ˆë©´ ValueError.
            raise ValueError("d_model must be divisible by num_heads.")

        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        # "Attention is All You Need" ë…¼ë¬¸ì—ì„œëŠ” Q, K, V í”„ë¡œì ì…˜ì— biasë¥¼ ì‚¬ìš© X.
        self.w_q = nn.Linear(d_model, d_model, bias=False) # bias : y = xW^T (+ b) (Trueë©´ + b)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)

        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()

    def _split_heads(self, tensor: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, _ = tensor.size()
        tensor = tensor.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # batch -> seq_len -> num_heads -> head_dim ê¸°ì¡´ ìˆœì„œë¥¼
        # batch -> num_heads -> seq_len -> head_dim ìˆœì„œë¡œ ë°”ê¿”ì¤˜ì•¼í•¨.
        # Attention ê³„ì‚°ì—ì„œëŠ” head ë³„ë¡œ ë³‘ë ¬ ì—°ì‚°ì„ í•´ì¤˜ì•¼í•˜ê¸° ë•Œë¬¸ì— seq_lenë³´ë‹¤ num_headsê°€ ë¨¼ì € ìˆì–´ì•¼í•˜ê¸°ë•Œë¬¸.
        return tensor.transpose(1, 2)  # (batch, num_heads, seq_len, head_dim)

    def _combine_heads(self, tensor: torch.Tensor) -> torch.Tensor:
        # 1ï¸âƒ£ (batch, num_heads, seq_len, head_dim)
        batch_size, num_heads, seq_len, head_dim = tensor.size()
        # contiguous ğŸ‘‰ ë©”ëª¨ë¦¬ìƒì—ì„œ í…ì„œë¥¼ ì—°ì†ëœ(continuous) í˜•íƒœë¡œ ë‹¤ì‹œ ì •ë ¬(copy)í•´ì£¼ëŠ” í•¨ìˆ˜ì˜ˆìš”. transpose ë‹¤ìŒì— view í•´ì£¼ë ¤ë©´ contiguous í•´ì¤˜ì•¼í•¨.
        tensor = tensor.transpose(1, 2).contiguous()
        # 2ï¸âƒ£ ëª¨ë“  headë¥¼ concat (flatten) -> (batch, seq_len, d_model)
        # d_model = num_heads * head_dim : ëª¨ë“  headì˜ ê²°ê³¼(head_dim)ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°(d_model)ë¡œ ì´ì–´ë¶™ì´ëŠ” ê²ƒì…ë‹ˆë‹¤
        return tensor.view(batch_size, seq_len, num_heads * head_dim)

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        q = self._split_heads(self.w_q(query))
        k = self._split_heads(self.w_k(key))
        v = self._split_heads(self.w_v(value))

        # scale dot product attention
        attn_output, attn_weights = self.attention(q, k, v, mask=mask)

        # concat attention
        attn_output = self._combine_heads(attn_output)
        attn_output = self.out_proj(attn_output)
        
        # ì„œë¸Œë ˆì´ì–´ ì¶œë ¥ì— dropout ì ìš©
        # attn_output = self.dropout(attn_output)
        return attn_output, attn_weights