#!/usr/bin/env python
# coding: utf-8

from transformers import AutoTokenizer
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import math

class MultiHeadAttention:
    def __init__(self, text, masked = False):
        self.masked = masked

        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        self.d_model = 4                         # ë…¼ë¬¸ ê¸°ì¤€ì€ 512

        self.tokens = self.tokenizer_text(text)
        self.X = self.embedding_tokens(self.tokens)

        self.max_position = len(self.tokens["input_ids"][0])  # Maximum sequence length

        self.pe = self.positional_encoding()
        # [[0,1,2,3], [a,b,c,d], [e,f,g,h], ...] : 4ê°œì˜ ì°¨ì›ì´ í† í° ê°œìˆ˜ë§Œí¼ ì¡´ìž¬
        # 0ë²ˆì§¸ í† í°ì˜ PE ê°’ë“¤ : [0, a, e, ...]
        print("Positional Encoding : ", self.pe)

        # Positional Encoding + Embedding
        pe_tensor = torch.tensor(self.pe, dtype=self.X.dtype)
        print(self.X.shape, pe_tensor.shape)
        pe_tensor.unsqueeze_(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ -> (1, token_len, d_model)
        print(self.X.shape, pe_tensor.shape)

        self.X_input = self.X + pe_tensor
        print("X_input : ", self.X_input)

        self.output = self.attention_input(self.X_input)
        self.out = self.concat_attention(self.output)

    def tokenizer_text(self, text: str):
        # 1ï¸âƒ£ í† í°í™” + ìˆ«ìží™”
        text = "I like coffee in the morning because it helps me wake up and stay focused."
        tokens = self.tokenizer(text, return_tensors="pt") # pt : pytorch, tf : TensorFlow
        token_ids = tokens["input_ids"][0]

        print("í† í° ëª©ë¡:", self.tokenizer.tokenize(text))
        print("í† í° ID:", token_ids)
        for t in token_ids:
            print(f"{t}\t -> {self.tokenizer.decode([t])}")    

        return tokens

    def embedding_tokens(self, tokens):
        # 2ï¸âƒ£ ìž„ë² ë”©
        vocab_size = self.tokenizer.vocab_size   # ì•½ 30,000ê°œ ë‹¨ì–´

        # Embedding : tokenì„ tensor íƒ€ìž…ìœ¼ë¡œ ë„£ì–´ì•¼í•¨. 
        embedding = nn.ModuleDict({
                "token_embedding" : nn.Embedding(vocab_size, self.d_model)
                })

        print(embedding.token_embedding)
        X = embedding.token_embedding(tokens["input_ids"])  # shape: (1, token_len, d_model)

        print("ìž„ë² ë”© ë²¡í„° :", X)  # torch
        print("ìž„ë² ë”© ë²¡í„° í¬ê¸°:", X.shape)  # torch.Size([1, 5, 4])

        return X


    def positional_encoding(self):
        position = np.arange(self.max_position)[:, np.newaxis] # [[0], [1], [2], ... , [max_position-1]]
        # The original formula pos / 10000^(2i/d_model) is equivalent to pos * (1 / 10000^(2i/d_model)).
        # I use the below version for numerical stability

        # `np.arange(0, d_model, 2)` : [0, 2, ...] -> ì§ìˆ˜ì—ëŒ€í•œ ê°’. d_modelì´ 4ë¼ë©´ [0, 2]
        # `np.log(10000.0)`          : ë¡œê·¸ ë³€í™˜ 
        # `np.exp(...)`              : ì§€ìˆ˜ í•¨ìˆ˜ (exponential)ë¡œ ì›ëž˜ ë¹„ìœ¨ ë³µì›
        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model))

        pe = np.zeros((self.max_position, self.d_model))     # d_model ì°¨ì›ë§Œí¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ í–‰ë ¬ ìƒì„±

        # `0::2` : ì§ìˆ˜ ì°¨ì› (0, 2, 4, â€¦) -> sin íŒŒí˜•
        # `1::2` : í™€ìˆ˜ ì°¨ì› (1, 3, 5, â€¦) -> cos íŒŒí˜•
        # `position * div_term` : ê° ìœ„ì¹˜ë§ˆë‹¤ ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ë§ ê³±
        # 
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)

        return pe

    def attention_input(self, X_input):
        # ìž…ë ¥ í† í° ìž„ë² ë”©ì„ Query ê³µê°„ìœ¼ë¡œ ì„ í˜• ë³€í™˜
        w_Q = nn.Linear(self.d_model, self.d_model) # y = xW^T + b
        w_K = nn.Linear(self.d_model, self.d_model)
        w_V = nn.Linear(self.d_model, self.d_model)

        # ì„ í˜•ë³€í™˜ìœ¼ë¡œ Q, K, V ìƒì„±
        Q = w_Q(X_input)  # shape: (1, token_len, d_model)
        K = w_K(X_input)
        V = w_V(X_input)

        # Q = Q.view(batch, seq_len, num_heads, head_dim)
        # batch : í•œë²ˆì˜ í•™ìŠµì—ì„œ ëª¨ë¸ì— ë™ì‹œì— ë„£ëŠ” ë°ì´í„° ë¬¶ìŒ. 1ê°œë©´ í† í° 1ê°œ, 32ê°œë©´ í† í° 32ê°œ.
        # num_heads : multi-head attentionì—ì„œ headì˜ ê°œìˆ˜
        # default num_heads = 8 or 12,,, ì—¬ê¸°ì„  8ë¡œ ì„¤ì •.

        print("Q shape:", Q.shape, "max_position : ", self.max_position)  # torch.Size([1, token_len, d_model]) : (ë°°ì¹˜ í¬ê¸°, í† í° ê¸¸ì´, ìž„ë² ë”© ì°¨ì›) -> Qì˜ ì›ì†Œ ìˆ˜ : 1 * token_len * d_model
        num_heads = 2 # d_modelì€ ë°˜ë“œì‹œ num_headsë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.
        head_dim = self.d_model // num_heads  # 4 // 2 -> 2

        # viewê°€ ë§Œë“¤ì–´ë‚´ëŠ” ì´ ì›ì†Œ ìˆ˜ëŠ” batch * seq_len * num_heads * head_dim -> Qì˜ ì›ì†Œ ìˆ˜ì™€ ë™ì¼í•´ì•¼í•œë‹¤.
        view_Q = Q.view(1, self.max_position, num_heads, head_dim)  # (batch, token_len, num_heads, head_dim)
        view_K = K.view(1, self.max_position, num_heads, head_dim)
        view_V = V.view(1, self.max_position, num_heads, head_dim)
        print("view_Q shape:", view_Q.shape)  # torch.Size([1, token_len, 1, d_model])

        # batch -> seq_len -> num_heads -> head_dim ìˆœì„œë¥¼
        # batch -> num_heads -> seq_len -> head_dim ìˆœì„œë¡œ ë°”ê¿”ì¤˜ì•¼í•¨.
        # Attention ê³„ì‚°ì—ì„œëŠ” head ë³„ë¡œ ë³‘ë ¬ ì—°ì‚°ì„ í•´ì¤˜ì•¼í•˜ê¸° ë•Œë¬¸ì— seq_lenë³´ë‹¤ num_headsê°€ ë¨¼ì € ìžˆì–´ì•¼í•˜ê¸°ë•Œë¬¸.
        transposed_Q = view_Q.transpose(1, 2)
        transposed_K = view_K.transpose(1, 2)
        transposed_V = view_V.transpose(1, 2)
        print("transposed_Q shape:", transposed_Q.shape) 

        scores = torch.matmul(transposed_Q, transposed_K.transpose(-2, -1))
        # ê²°ê³¼: (batch, num_heads, seq_len, seq_len) -> (ëª‡ê°œì˜ ë¬¸ìž¥ì¸ì§€, ëª‡ê°œì˜ headì¸ì§€, Q í† í° ìœ„ì¹˜, K í† í° ìœ„ì¹˜)
        print("scores shape:", scores.shape)
        
        if self.masked:
            # decoderì—ì„œ ì ìš©í•´ì•¼ë˜ëŠ” ë¶€ë¶„, scores.masked_fill(0) maskê°€ Trueì¸ ìœ„ì¹˜ë¥¼ valueë¡œ ì±„ì›Œ ë„£ì–´ ë¬´ì‹œë˜ê²Œ í•œë‹¤. ì´ì „ ë°ì´í„°ë§Œ ì¸ì‹í•  ìˆ˜ ìžˆë„ë¡!
            mask = torch.tril(torch.ones(self.max_position, self.max_position))  # í•˜ì‚¼ê°í–‰ë ¬ : í–‰ë ¬ì˜ ëŒ€ê°ì„  ì•„ëž˜ìª½ë¶€ë¶„ë§Œ 1ë¡œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“¤ê²Œ í•¨.
            print(mask)
            scores = scores.masked_fill(mask == 0, float('-inf'))
            print(scores)

        # Q * K^T / sqrt(d_k)
        scores = scores / math.sqrt(head_dim)
        print("Scaled Scores shape:", scores.shape)

        # softmax(Q * K^T / sqrt(d_k))
        # softmax ì ìš© (dim=-1 : ë§ˆì§€ë§‰ ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ softmax ì ìš© -> ê° queryì˜ attention weight í•©ì´ 1ì´ ë˜ê²Œ ë§Œë“¦)
        # attentionì—ì„  dimì„ -1ë¡œ ì£¼ë¡œ ì„¤ì •í•¨. ì´ìœ ëŠ” ë§ˆì§€ë§‰ ì°¨ì›ì´ key í† í° ìœ„ì¹˜ì— í•´ë‹¹í•˜ê¸° ë•Œë¬¸.
        # scoresì˜ ë§ˆì§€ë§‰ ì°¨ì›ì€ K í† í° ìœ„ì¹˜ì— í•´ë‹¹.
        attention_weights = torch.softmax(scores, dim=-1)
        print("Attention Weights shape :", attention_weights.shape)

        # softmax(Q * K^T / sqrt(d_k)) * V
        output = torch.matmul(attention_weights, transposed_V)
        print("Output shape:", output.shape)  # (batch, num_heads, seq_len, head_dim)

        return output

    def concat_attention(self, output):
        # 1ï¸âƒ£ (batch, num_heads, seq_len, head_dim)
        # contiguous ðŸ‘‰ ë©”ëª¨ë¦¬ìƒì—ì„œ í…ì„œë¥¼ ì—°ì†ëœ(continuous) í˜•íƒœë¡œ ë‹¤ì‹œ ì •ë ¬(copy)í•´ì£¼ëŠ” í•¨ìˆ˜ì˜ˆìš”. transpose ë‹¤ìŒì— view í•´ì£¼ë ¤ë©´ contiguous í•´ì¤˜ì•¼í•¨.
        transposed_O = output.transpose(1, 2).contiguous()
        #  -> (batch, seq_len, num_heads, head_dim)

        batch, num_heads, seq_len, head_dim = output.shape

        # 2ï¸âƒ£ ëª¨ë“  headë¥¼ concat (flatten)
        out = transposed_O.view(batch, seq_len, self.d_model)
        #  -> (batch, seq_len, d_model)
        print("Final Output shape:", out.shape)  # (batch, seq_len, d_model)

        return out


if __name__ == "__main__":
    text = "I like coffee in the morning because it helps me wake up and stay focused."
    mha = MultiHeadAttention(text)  
    print(mha.out)