{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decoder\n",
        "인코더 출력과 teacher forcing으로 들어온 이전 토큰들을 받아 다음 토큰 확률을 예측하는 Transformer 디코더를 순차적으로 살펴봅니다.\n",
        "- Masked Multi-Head Self-Attention (look-ahead mask)\n",
        "- Encoder-Decoder Multi-Head Attention\n",
        "- Position-wise FeedForward Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "디코더 토큰 목록: ['[CLS]', 'i', 'like', 'coffee', 'in', 'the', 'morning', 'because', 'it', 'helps', 'me', 'wake', 'up', 'and', 'stay', 'focused', '.', '[SEP]']\n",
            "디코더 토큰 ID: tensor([[ 101, 1045, 2066, 4157, 1999, 1996, 2851, 2138, 2009, 7126, 2033, 5256,\n",
            "         2039, 1998, 2994, 4208, 1012,  102]])\n",
            "디코더 타깃 ID(shifted): tensor([ 101, 1045, 2066, 4157, 1999, 1996, 2851, 2138, 2009, 7126, 2033, 5256,\n",
            "        2039, 1998, 2994, 4208, 1012,  102])\n",
            "101\t -> [CLS]\n",
            "1045\t -> i\n",
            "2066\t -> like\n",
            "4157\t -> coffee\n",
            "1999\t -> in\n",
            "1996\t -> the\n",
            "2851\t -> morning\n",
            "2138\t -> because\n",
            "2009\t -> it\n",
            "7126\t -> helps\n",
            "2033\t -> me\n",
            "5256\t -> wake\n",
            "2039\t -> up\n",
            "1998\t -> and\n",
            "2994\t -> stay\n",
            "4208\t -> focused\n",
            "1012\t -> .\n",
            "102\t -> [SEP]\n"
          ]
        }
      ],
      "source": [
        "# 1️⃣ 디코더 입력 토큰화 + 숫자화\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# teacher forcing: 이전 정답 토큰들을 디코더 입력으로 사용\n",
        "target_text = \"I like coffee in the morning because it helps me wake up and stay focused.\"\n",
        "decoder_tokens = tokenizer(target_text, return_tensors=\"pt\")\n",
        "decoder_input_ids = decoder_tokens[\"input_ids\"]\n",
        "decoder_target_ids = decoder_input_ids[0]\n",
        "\n",
        "print(\"디코더 토큰 목록:\", tokenizer.convert_ids_to_tokens(decoder_input_ids[0]))\n",
        "print(\"디코더 토큰 ID:\", decoder_input_ids)\n",
        "print(\"디코더 타깃 ID(shifted):\", decoder_target_ids)\n",
        "for t in decoder_input_ids[0]:\n",
        "    print(f\"{t.item()}\\t -> {tokenizer.decode([t.item()])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(30522, 4)\n",
            "디코더 임베딩 텐서: tensor([[[-1.5208, -0.5045, -0.4309,  0.7027],\n",
            "         [ 0.0796,  1.6245,  0.2432,  0.6824],\n",
            "         [ 0.7327,  0.9034,  0.6517, -0.1352],\n",
            "         [-0.9510,  0.6579,  0.1555,  2.3967],\n",
            "         [ 0.9939,  0.0868, -0.9186, -1.0484],\n",
            "         [ 1.3002,  1.1354, -0.0370, -0.0625],\n",
            "         [-0.1231,  1.9497,  1.5360, -1.7829],\n",
            "         [-0.0634,  0.9844,  1.5397,  0.4743],\n",
            "         [-0.9818,  0.3095,  0.0478, -0.5977],\n",
            "         [ 0.9356,  0.5767,  0.0420,  0.7946],\n",
            "         [-0.6358,  1.7854, -1.1140, -2.7240],\n",
            "         [ 0.1240,  1.7443, -0.0137, -0.1124],\n",
            "         [ 1.3834,  0.8058, -1.2538, -0.0740],\n",
            "         [ 0.6900,  0.4595,  2.1793,  0.8026],\n",
            "         [ 0.4543, -2.0235, -0.8490,  0.6042],\n",
            "         [-0.5373, -1.2456,  1.1772,  0.6860],\n",
            "         [-0.1718,  0.8760, -0.2489,  0.6409],\n",
            "         [ 2.4583, -0.3241, -2.2090, -1.4700]]], grad_fn=<EmbeddingBackward0>)\n",
            "디코더 임베딩 크기: torch.Size([1, 18, 4])\n"
          ]
        }
      ],
      "source": [
        "# 2️⃣ 임베딩 (Decoder input embedding)\n",
        "vocab_size = tokenizer.vocab_size   # 약 30,000개 단어\n",
        "d_model = 4                         # 논문 기준은 512\n",
        "\n",
        "decoder_embedding = nn.ModuleDict({\n",
        "    \"token_embedding\": nn.Embedding(vocab_size, d_model)\n",
        "})\n",
        "\n",
        "print(decoder_embedding.token_embedding)\n",
        "decoder_X = decoder_embedding.token_embedding(decoder_input_ids)\n",
        "\n",
        "print(\"디코더 임베딩 텐서:\", decoder_X)\n",
        "print(\"디코더 임베딩 크기:\", decoder_X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Positional Encoding\n",
        "\n",
        "$$ PE(pos,2i)=sin(\\frac{pos} {10000^{2i/d_{model}}}) $$\n",
        "$$ PE(pos,2i+1)=cos(\\frac{pos} {10000^{2i/d_{model}}}) $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "디코더 Positional Encoding: [[ 0.          1.          0.          1.        ]\n",
            " [ 0.84147098  0.54030231  0.00999983  0.99995   ]\n",
            " [ 0.90929743 -0.41614684  0.01999867  0.99980001]\n",
            " [ 0.14112001 -0.9899925   0.0299955   0.99955003]\n",
            " [-0.7568025  -0.65364362  0.03998933  0.99920011]\n",
            " [-0.95892427  0.28366219  0.04997917  0.99875026]\n",
            " [-0.2794155   0.96017029  0.05996401  0.99820054]\n",
            " [ 0.6569866   0.75390225  0.06994285  0.997551  ]\n",
            " [ 0.98935825 -0.14550003  0.07991469  0.99680171]\n",
            " [ 0.41211849 -0.91113026  0.08987855  0.99595273]\n",
            " [-0.54402111 -0.83907153  0.09983342  0.99500417]\n",
            " [-0.99999021  0.0044257   0.1097783   0.9939561 ]\n",
            " [-0.53657292  0.84385396  0.11971221  0.99280864]\n",
            " [ 0.42016704  0.90744678  0.12963414  0.99156189]\n",
            " [ 0.99060736  0.13673722  0.13954311  0.990216  ]\n",
            " [ 0.65028784 -0.75968791  0.14943813  0.98877108]\n",
            " [-0.28790332 -0.95765948  0.15931821  0.98722728]\n",
            " [-0.96139749 -0.27516334  0.16918235  0.98558477]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def positional_encoding(max_position, d_model):\n",
        "    position = np.arange(max_position)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "\n",
        "    pe = np.zeros((max_position, d_model))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "decoder_seq_len = decoder_input_ids.size(1)\n",
        "pe = positional_encoding(decoder_seq_len, d_model)\n",
        "print(\"디코더 Positional Encoding:\", pe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional Encoding + Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 18, 4]) torch.Size([18, 4])\n",
            "torch.Size([1, 18, 4]) torch.Size([1, 18, 4])\n",
            "Positional Encoding이 더해진 디코더 입력: torch.Size([1, 18, 4])\n"
          ]
        }
      ],
      "source": [
        "pe_tensor = torch.tensor(pe, dtype=decoder_X.dtype)\n",
        "print(decoder_X.shape, pe_tensor.shape)\n",
        "pe_tensor.unsqueeze_(0)\n",
        "print(decoder_X.shape, pe_tensor.shape)\n",
        "decoder_X_input = decoder_X + pe_tensor\n",
        "print(\"Positional Encoding이 더해진 디코더 입력:\", decoder_X_input.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Masked Multi-Head Self-Attention\n",
        "디코더는 미래 토큰을 보지 못하도록 상삼각 마스크(look-ahead mask)를 적용한 상태에서 멀티-헤드 어텐션을 수행합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[-2.5858e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 6.5546e-01,  2.0132e-01,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 6.0399e-01,  2.4881e-01, -1.4350e-01,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 9.3077e-02, -9.8278e-02, -3.2322e-01, -5.3880e-01,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [-2.9617e-01, -7.5903e-02,  1.7198e-01, -7.0812e-02, -2.6560e-01,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 3.0001e-01,  7.7603e-02, -1.7263e-01,  7.5021e-02,  2.6642e-01,\n",
            "            1.2345e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 5.4483e-01,  1.2295e-01, -3.5313e-01,  5.3653e-02,  5.4971e-01,\n",
            "            2.2902e-01, -4.9213e-01,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 8.2496e-01,  2.2833e-01, -4.4177e-01,  2.7493e-01,  6.7782e-01,\n",
            "            3.3546e-01, -6.2946e-01, -7.8829e-01,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 3.2762e-02, -9.2823e-03, -5.7987e-02, -7.3379e-02,  9.4172e-02,\n",
            "            1.8246e-02, -7.5355e-02, -1.4067e-01, -1.1341e-04,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 3.9224e-01,  1.5565e-01, -1.0627e-01,  3.4703e-01,  1.4970e-01,\n",
            "            1.4687e-01, -1.7007e-01, -9.4172e-02,  5.6136e-02,  5.9261e-02,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [-5.9402e-01, -2.2397e-01,  1.8684e-01, -4.7155e-01, -2.6980e-01,\n",
            "           -2.2557e-01,  2.8981e-01,  2.1267e-01, -8.2482e-02, -6.7513e-02,\n",
            "           -3.6521e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 9.4698e-02, -5.9456e-02, -2.3952e-01, -3.6198e-01,  3.9178e-01,\n",
            "            6.1493e-02, -3.0735e-01, -6.0105e-01, -7.3590e-03, -1.6923e-01,\n",
            "            6.9433e-01,  4.6886e-02,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 1.6970e-01,  7.6948e-02, -2.4809e-02,  1.9427e-01,  2.9568e-02,\n",
            "            6.0965e-02, -4.7223e-02,  1.6500e-02,  2.6358e-02,  4.3807e-02,\n",
            "            1.7683e-02,  9.9930e-02,  9.3146e-02,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 1.0824e+00,  3.2759e-01, -5.1792e-01,  4.8936e-01,  7.8672e-01,\n",
            "            4.3264e-01, -7.4905e-01, -8.6741e-01,  1.3294e-01, -2.9278e-02,\n",
            "            1.2037e+00,  6.2322e-01,  1.3599e+00, -1.5800e+00,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [-1.3212e-02,  5.4667e-02,  1.3562e-01,  2.6353e-01, -2.2461e-01,\n",
            "           -2.1021e-02,  1.7014e-01,  3.6023e-01,  1.1020e-02,  1.1132e-01,\n",
            "           -4.0685e-01, -2.5210e-03, -2.9187e-01,  5.4519e-01, -4.1266e-02,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 1.6794e-01,  1.2134e-02, -1.6564e-01, -1.0182e-01,  2.6388e-01,\n",
            "            7.7508e-02, -2.2240e-01, -3.6519e-01,  1.2288e-02, -7.7730e-02,\n",
            "            4.4541e-01,  9.3343e-02,  3.9255e-01, -5.9201e-01,  8.6756e-02,\n",
            "           -4.5305e-01,        -inf,        -inf],\n",
            "          [-3.3500e-02, -7.9421e-02, -1.3667e-01, -3.3342e-01,  2.2957e-01,\n",
            "            5.1995e-03, -1.6694e-01, -3.8607e-01, -1.9046e-02, -1.3014e-01,\n",
            "            4.2587e-01, -2.5295e-02,  2.8300e-01, -5.7218e-01,  3.0373e-02,\n",
            "           -4.6774e-01, -9.8006e-02,        -inf],\n",
            "          [-3.4234e-01,  4.1384e-03,  4.0127e-01,  3.4020e-01, -6.4371e-01,\n",
            "           -1.6574e-01,  5.3258e-01,  9.1649e-01, -1.8826e-02,  2.1306e-01,\n",
            "           -1.1009e+00, -1.8777e-01, -9.3566e-01,  1.4656e+00, -1.9472e-01,\n",
            "            1.1328e+00, -3.0558e-02, -1.4132e+00]],\n",
            "\n",
            "         [[ 5.3917e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 2.5103e-01,  1.4747e-01,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [-1.4426e-01,  1.2123e-02, -1.9889e-02,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 3.8544e-01,  9.5018e-02,  4.3692e-02,  1.0994e+00,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [-1.6395e-01, -6.2820e-02, -1.6924e-02, -3.3590e-01,  1.1202e-02,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 2.3238e-01,  1.2167e-01,  2.1568e-02,  2.8421e-01, -8.6481e-02,\n",
            "            9.8193e-02,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 7.4647e-01,  3.7628e-01,  7.0362e-02,  9.9853e-01, -2.4631e-01,\n",
            "            2.8711e-01,  7.5997e-01,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 4.2436e-01,  2.2985e-01,  3.8818e-02,  4.7392e-01, -1.7451e-01,\n",
            "            1.9423e-01,  7.0195e-01,  5.6707e-01,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 1.3152e-01,  6.7808e-02,  1.2286e-02,  1.6706e-01, -4.6666e-02,\n",
            "            5.3525e-02,  1.5947e-01,  1.5494e-01,  4.7008e-02,        -inf,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [-2.1532e-01, -5.7867e-02, -2.4054e-02, -5.8604e-01, -3.8601e-02,\n",
            "            1.5769e-02,  6.3896e-01,  6.8923e-02,  2.8119e-02, -3.1953e-01,\n",
            "                  -inf,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 3.1768e-01,  1.2776e-01,  3.2346e-02,  6.1539e-01, -3.4759e-02,\n",
            "            5.9192e-02, -2.2495e-01,  1.5556e-01,  4.2312e-02,  2.2899e-01,\n",
            "           -3.1272e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 5.8682e-01,  2.3334e-01,  5.9945e-02,  1.1523e+00, -5.8469e-02,\n",
            "            1.0418e-01, -4.6042e-01,  2.7126e-01,  7.2916e-02,  4.3814e-01,\n",
            "           -5.9927e-01,  1.7282e-01,        -inf,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 3.6373e-03,  2.7452e-02, -1.5567e-03, -1.4580e-01, -5.6637e-02,\n",
            "            5.1242e-02,  4.3757e-01,  1.5953e-01,  5.1871e-02, -1.4605e-01,\n",
            "            2.0832e-01,  1.4385e-01, -6.4985e-02,        -inf,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [ 3.3981e-01,  2.1482e-01,  2.8803e-02,  1.9855e-01, -2.0633e-01,\n",
            "            2.1539e-01,  1.0832e+00,  6.4085e-01,  1.9981e-01, -2.0222e-01,\n",
            "            3.0281e-01,  5.3764e-01, -1.9942e-01,  6.7867e-01,        -inf,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [-5.6427e-01, -2.2378e-01, -5.7687e-02, -1.1115e+00,  5.4938e-02,\n",
            "           -9.9023e-02,  4.5279e-01, -2.5723e-01, -6.8940e-02, -4.2471e-01,\n",
            "            5.8109e-01, -1.6292e-01, -1.0495e-03, -3.6079e-01, -4.6301e-01,\n",
            "                  -inf,        -inf,        -inf],\n",
            "          [-8.2551e-03, -6.9598e-03, -5.7061e-04,  5.4163e-03,  8.7800e-03,\n",
            "           -8.6201e-03, -5.5801e-02, -2.6137e-02, -8.2966e-03,  1.4873e-02,\n",
            "           -2.1552e-02, -2.2620e-02,  9.1952e-03, -2.6522e-02,  3.5422e-02,\n",
            "           -1.4922e-03,        -inf,        -inf],\n",
            "          [ 2.3379e-01,  6.9536e-02,  2.5620e-02,  5.9688e-01,  2.7403e-02,\n",
            "           -4.0765e-03, -5.8022e-01, -3.4137e-02, -1.7273e-02,  3.0859e-01,\n",
            "           -4.2978e-01, -5.9775e-02,  5.8364e-02,  1.5877e-02,  4.5722e-01,\n",
            "            3.2779e-01,  2.5346e-01,        -inf],\n",
            "          [-6.3346e-01, -2.2779e-01, -6.6497e-02, -1.3856e+00,  1.0970e-02,\n",
            "           -6.5576e-02,  9.0514e-01, -1.4655e-01, -3.1064e-02, -6.1083e-01,\n",
            "            8.4339e-01, -5.4251e-02, -5.9728e-02, -2.6999e-01, -7.8801e-01,\n",
            "           -8.0000e-01, -6.1185e-01,  1.9115e-01]]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "Masked Self-Attention 출력 형태: torch.Size([1, 18, 4])\n"
          ]
        }
      ],
      "source": [
        "num_heads = 2  # d_model은 num_heads로 나누어 떨어져야 합니다.\n",
        "head_dim = d_model // num_heads\n",
        "\n",
        "self_attention = nn.ModuleDict({\n",
        "    \"w_q\": nn.Linear(d_model, d_model),\n",
        "    \"w_k\": nn.Linear(d_model, d_model),\n",
        "    \"w_v\": nn.Linear(d_model, d_model),\n",
        "    \"w_o\": nn.Linear(d_model, d_model) # 출력 투영(output projection)\n",
        "})\n",
        "\n",
        "batch_size, seq_len, _ = decoder_X_input.shape\n",
        "Q = self_attention[\"w_q\"](decoder_X_input).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "K = self_attention[\"w_k\"](decoder_X_input).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "V = self_attention[\"w_v\"](decoder_X_input).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "# Q * K^T / sqrt(d_k)\n",
        "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)\n",
        "\n",
        "# Decoder Masked\n",
        "look_ahead_mask = torch.triu(torch.ones(seq_len, seq_len, device=scores.device), diagonal=1).bool()\n",
        "look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(0)\n",
        "scores = scores.masked_fill(look_ahead_mask, float('-inf'))\n",
        "\n",
        "# print(scores)\n",
        "\n",
        "# softmax(Q * K^T / sqrt(d_k))\n",
        "self_attn_weights = torch.softmax(scores, dim=-1)\n",
        "# softmax(Q * K^T / sqrt(d_k)) * V\n",
        "self_attn_output = torch.matmul(self_attn_weights, V)\n",
        "\n",
        "self_attn_output = self_attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "self_attn_output = self_attention[\"w_o\"](self_attn_output)\n",
        "print(\"Masked Self-Attention 출력 형태:\", self_attn_output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add & Norm\n",
        "Residual connection과 LayerNorm으로 서브레이어 출력을 안정화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-Attention Add & Norm 결과 형태: torch.Size([1, 11, 4])\n"
          ]
        }
      ],
      "source": [
        "def add_norm(x, sublayer_out, d_model, dropout_rate=0.1):\n",
        "    norm = nn.LayerNorm(d_model)\n",
        "    dropout = nn.Dropout(dropout_rate)\n",
        "    return norm(x + dropout(sublayer_out))\n",
        "\n",
        "decoder_self_residual = add_norm(decoder_X_input, self_attn_output, d_model)\n",
        "print(\"Self-Attention Add & Norm 결과 형태:\", decoder_self_residual.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoder-Decoder Attention\n",
        "인코더 출력(`memory`)을 Key/Value로 사용해 입력 문장과 디코더 현재 상태를 연결합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder-Decoder Attention Add & Norm 결과 형태: torch.Size([1, 11, 4])\n"
          ]
        }
      ],
      "source": [
        "encoder_seq_len = 6\n",
        "encoder_memory = torch.randn(batch_size, encoder_seq_len, d_model)  # 실제로는 인코더 출력이 들어옵니다.\n",
        "\n",
        "cross_attention = nn.ModuleDict({\n",
        "    \"w_q\": nn.Linear(d_model, d_model),\n",
        "    \"w_k\": nn.Linear(d_model, d_model),\n",
        "    \"w_v\": nn.Linear(d_model, d_model),\n",
        "    \"w_o\": nn.Linear(d_model, d_model)\n",
        "})\n",
        "\n",
        "Q_cross = cross_attention[\"w_q\"](decoder_self_residual).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "K_cross = cross_attention[\"w_k\"](encoder_memory).view(batch_size, encoder_seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "V_cross = cross_attention[\"w_v\"](encoder_memory).view(batch_size, encoder_seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "cross_scores = torch.matmul(Q_cross, K_cross.transpose(-2, -1)) / math.sqrt(head_dim)\n",
        "encoder_padding_mask = torch.ones(encoder_seq_len, device=cross_scores.device, dtype=torch.bool)\n",
        "encoder_padding_mask[-1] = False  # 패딩 토큰 예시\n",
        "encoder_padding_mask = encoder_padding_mask.view(1, 1, 1, encoder_seq_len)\n",
        "cross_scores = cross_scores.masked_fill(~encoder_padding_mask, float('-inf'))\n",
        "\n",
        "cross_attn_weights = torch.softmax(cross_scores, dim=-1)\n",
        "cross_attn_output = torch.matmul(cross_attn_weights, V_cross)\n",
        "cross_attn_output = cross_attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "cross_attn_output = cross_attention[\"w_o\"](cross_attn_output)\n",
        "\n",
        "decoder_cross_residual = add_norm(decoder_self_residual, cross_attn_output, d_model)\n",
        "print(\"Encoder-Decoder Attention Add & Norm 결과 형태:\", decoder_cross_residual.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FeedForward Network\n",
        "각 토큰별로 독립적인 2층 완전연결 네트워크로 표현력을 확장합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForward Add & Norm 결과 형태: torch.Size([1, 11, 4])\n"
          ]
        }
      ],
      "source": [
        "dropout_rate = 0.1\n",
        "d_ff = 4 * d_model\n",
        "\n",
        "decoder_ffn = nn.Sequential(\n",
        "    nn.Linear(d_model, d_ff),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(dropout_rate),\n",
        "    nn.Linear(d_ff, d_model),\n",
        ")\n",
        "\n",
        "ffn_output = decoder_ffn(decoder_cross_residual)\n",
        "decoder_output = add_norm(decoder_cross_residual, ffn_output, d_model)\n",
        "print(\"FeedForward Add & Norm 결과 형태:\", decoder_output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output Projection\n",
        "최종적으로 vocab 차원으로 사상한 뒤 softmax로 다음 토큰 확률을 계산합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "로짓 텐서 shape: torch.Size([1, 11, 30522])\n",
            " 5758 -> memories (prob=0.0002)\n",
            "11925 -> contacted (prob=0.0002)\n",
            "12861 -> 1809 (prob=0.0002)\n",
            " 4858 -> finds (prob=0.0002)\n",
            "10628 -> curiosity (prob=0.0002)\n"
          ]
        }
      ],
      "source": [
        "projection = nn.Linear(d_model, vocab_size)\n",
        "logits = projection(decoder_output)\n",
        "print(\"로짓 텐서 shape:\", logits.shape)\n",
        "\n",
        "next_token_probs = torch.softmax(logits[:, -1], dim=-1)\n",
        "topk = torch.topk(next_token_probs, k=5)\n",
        "for idx, prob in zip(topk.indices[0], topk.values[0]):\n",
        "    print(f\"{idx.item():5d} -> {tokenizer.decode([idx.item()])} (prob={prob.item():.4f})\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "translator-1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
